# External control software
This repository contains code for the followings:

1. Windows form app to command the visual servoing experiment.
2. Python code to carry out reinforcement learning.

# Solution structure

* The *FanucController*  folder contains code for:
  * Visual Studio *solution* file.
  * Windows form design.
  * C-Track interface.
  * Real-time controllers (PID, ILC, ANNPID & RL-based).
  * Real-time filter (Kalman filter & robust Kalman filter).
  * Neural network inference with ONNX.
* The *FanucPCDK*  folder contains code for:
  * FANUC interface with PCDK.
  * Dynamic path modification (DPM).
* The *PythonMBPO* folder contains code for:
  * Offline policy training with the TD3 algorithm.

# Form Design

![](D:\LocalRepos\dotnet-fanuc-controller\form.PNG)

# Tutorials

## (1) Setting up a file structure to store experimental data

The external control software works when both input and output files are organized in the following structure:

* **Top** `\exp`: The top directory contains all folders and files concerning an experiment.
  * **Reference** `\exp\reference`: The reference directory contains:
    * Model file and targets file (generated by VXelements) necessary for object tracking with the C-Track (*model_####.txt* & *targets_####.txt*)
    * JSON file describing the line path to be followed. (*LinearPath.json*)
    * JSON file describing the pose transformation between sensor and robot frames. (*RotationId.json*)
  * **Output** `\exp\output`: The output directory contains:
    * Iteration `\exp\output\iteration_#`: The iteration directory contains:
      * `actor_xyz.onnx` & `actor_wpr.onnx`: actor network (ONNX)
      * `best_actor_xyz.pt` & `best_actor_wpr.pt`: actor network (Pytorch)
      * `best_critic_xyz.pt` & `best_critic_wpr.pt`: critic network (Pytorch)
      * `combined_buffer_xyz.csv` & `combined_buffer_wpr.csv`: state transition memory buffer (Pytorch)
      * `LineTrackControl.csv`: almagamated control signal
      * `LineTrackError.csv`: path error signal
      * `LineTrackKfError.csv`: path error signal filtered (Kalman Filter)
      * `LineTrackMbpoControl.csv`: RL-based control signal
      * `LineTrackPidControl.csv`: PID control signal
      * `LineTrackPose.csv`: end effector pose signal
      * `LineTrackPoseKf.csv`: end effector pose signal (Kalman filter)
      * `LineTrackPoseRaw.csv`: end effector pose signal (raw)
      * `LineTrackPoseRkf.csv`: end effector pose signal (robust Kalman filter)
      * `VxKf.csv`: estimated pose signal from C-Track (Kalman filter)
      * `VxRaw.csv`: estimated pose signal from C-Track (raw)
      * `VxRkf.csv`: estimated pose signal from C-Track (robust Kalman filter)
    * MBPO `\exp\output\mbpo`: The MBPO directory contains:

## (2) Tracking an object with C-Track

**Input**: 

* Model file
* Target files
* CSV file paths (raw, kf, rkf)

**Output**: 

* Recorded poses in CSV files (raw, kf, rkf)

**Steps**:

1. Specify the path to the model and targets files. (Lines 608 and 609 in *MainForm.cs*).
2. Check checkboxes to:
   * *Append*: Append each new measured pose to a specified CSV file.
   * *Raw*: Save raw data in a memory buffer.
   * *KF*: Enable kalman filter and save 
   * *RKF*:
3. Click on the *Quick Connect* button to connect to VXelements API, attach to VXelements events, load positioning targets, load tracking model and activate filters.
4. Click on the *Start Tracking* button to start tracking the object model and recording the measurements in a memory buffer.
5. Click on the *Display* button to display the real-time pose in the data grid.
6. Click on the *Stop Tracking* button to stop tracking.
7. Click on the *Export Buffer* button to export the memory buffer data to CSV files.
8. Click on the *Reset*  button to reset the memory buffer and connection to VXelements.
9. Click on the *Exit* button to disconnect from the VXelements.

## (3) Recording the FANUC robot pose

**Input**:

* FANUC robot IP address (192.168.0.10)
* CSV file paths (Cartesian & joint spaces)

**Output**:

* Recorded poses in CSV files (Cartesian & joint spaces)

**Steps**:

1. Click on the *Connect* button to connect to PCDK.
2. *Pose* group box:
   * Click on the *Get* button to update the display of the latest robot Cartesian pose.
   * Click on the *Attach* button to continuously update the pose display and save the obtained poses in a memory buffer.
   * On the combo box, select *User Frame* to display the pose in the user frame or *World Frame* to display the pose in the world frame. 
   * Click on the *Export* button to export the poses in the memory buffer to a CSV file.
   * Check the *A* checkbox to append each new measurement to the CSV file.
3. *Joint* group box:
   * Same as *Pose* group box.
4. *DPM* group box:
   * Select the DPM group (default: 1).
   * Specify the six Cartesian pose offsets in the corresponding text boxes (Keep the DPM offsets at small and safe values).
   * Click on the *Apply Offset* button to induce the offset.

## (4) Running a TP program remotely

**Input**: 

* TP program name

**Steps**:

1. Specify the program name in the textbox under the label *Program Name*.
2. Connect to PCDK.
3. *FANUC Experiment* group box:
   * Click on the *Run* button.

## (5) Setting up the pose transformation between sensor and robot frames

**Input**: 

* Model file
* Target files
* JSON file encoding the pose transformation

**Output**:

* JSON file encoding the pose transformation

**Steps**:

1. Follow tutorial (1) to track the robot end effector.

2. *Record Pose* group box:

   * Select *World Frame*.

   * Move the robot to an arbitrary position.
   * Select *idX1* under the *Pose Dict Keys* label and click on the *Record* button to record the start position.
   * Move the robot end effector in the positive *X* direction (user frame).
   * Select *idX2* under the *Pose Dict Keys* label and click on the *Record* button to record the end position.
   * Repeat the above steps for (*idY1, idY2*) and (*idZ1, idZ2*), corresponding to *Y* and *Z* directions in the robot user frame.

3. *Rotation ID* group box:

   1. Select either *xy, xz*, or *yz* under the *xyz* label.
   2. Click on the *Compute* button to compute the rotation matrix.
   3. Click on the *Compute Offset* button to compute the rotation and position offsets.
   4. Click on the *Write JSON* to export the pose transformation to a JSON file.
   5. Click on the *Load JSON* file to load the pose transformation from a JSON file.

## (6) Line following experiment

**Input**:

* Model file
* Target file
* TP program name
* JSON file encoding the pose transformation
* JSON file encoding the line start & end poses

**Output**:

* JSON file encoding the line start & end poses
* CSV files holding the experimental data.

**Steps**:

1. Set up file structure as discussed in the tutorial (1).
2. Start tracking the object model as discussed in the tutorial (2).
3. Specify the pose transformation as discussed in the tutorial (5)
4. Specify the line path parameters
   * Move the robot to the starting pose of the line path.
   * (*Record Pose*) Select *startP*  and press *Record*.
   * (*Record Pose*) Select *endP* and press *Record*.
   * (*Linear Path*) Press *Write JSON* to export the line path parameters to a JSON file.
   * (*Linear Path*) Press *Load JSON* button to load the line path parameters from a JSON file.
5. Select path following options:
   * *DPM*: Enable DPM offset.
   * *P Control*: Enable PID controller.
   * *ILC*: Enable ILC controller.
   * *P-NN*: Enable neural network predictive PID controller (don't use)
   * *MBPO*: Enable RL-based controller.
   * *BPNNPID*: Enable BPNNPID (Jianyu).
   * *Step Mode*: Enable step mode (wait for user input after each iteration).
   * *Position*: Enable position control.
   * *Orientation*: Enable orientation control.
   * *Line*: Line following experiment.
   * *Circle*: Circle following experiment (not implemented)
6. Specifiy the number of iterations next to the label *Iter:*.
7. Click on *Run* to start the line following experiment.



